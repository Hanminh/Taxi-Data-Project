{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import os\n",
    "import sys\n",
    "from env_variable import *\n",
    "from hdfs import InsecureClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, coalesce, lit, year, weekofyear, from_unixtime, unix_timestamp, expr, to_date, month, dayofmonth\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Set environment variables for Spark\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Read Data from HDFS') \\\n",
    "    .config('spark.default.parallelism', 100) \\\n",
    "    .config('spark.cassandra.connection.host', 'localhost') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Initialize HDFS client\n",
    "client = InsecureClient(HDFS_URL)\n",
    "\n",
    "# Retrieve list of files in the directory\n",
    "directory = client.list(HDFS_PATH)\n",
    "paths = [f\"{HDFS_NAMENODE_URL}{HDFS_PATH}{file}\" for file in directory]\n",
    "\n",
    "# Read the data files into a DataFrame\n",
    "df = spark.read.format('csv') \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(paths)\n",
    "\n",
    "# Rename relevant columns to lowercase\n",
    "column_mapping = {\n",
    "    \"VendorID\": \"vendorid\",\n",
    "    \"PULocationID\": \"pulocationid\",\n",
    "    \"DOLocationID\": \"dolocationid\",\n",
    "    \"RatecodeID\": \"ratecodeid\"\n",
    "}\n",
    "for old_col, new_col in column_mapping.items():\n",
    "    df = df.withColumnRenamed(old_col, new_col)\n",
    "\n",
    "# Filter for trips after January 1, 2018\n",
    "df = df.filter(df.tpep_pickup_datetime > '2018-01-01')\n",
    "\n",
    "# Read existing data from the Cassandra table\n",
    "df_cassandra = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"total_amount_statistics\", keyspace=\"statistics\") \\\n",
    "    .load()\n",
    "\n",
    "# Aggregate the total amount per day\n",
    "df_total_amount = df.groupBy(\n",
    "    year(\"tpep_dropoff_datetime\").alias(\"year\"),\n",
    "    month(\"tpep_dropoff_datetime\").alias(\"month\"),\n",
    "    dayofmonth(\"tpep_dropoff_datetime\").alias(\"day\")\n",
    ").agg({\"total_amount\": \"sum\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_total_amount.join(df_cassandra, ['year', 'month', 'day'], 'full')\\\n",
    "    .withColumn('final_amount', coalesce(col('sum(total_amount)')) + coalesce(col('total_amount'), lit(0)))\\\n",
    "        .drop('sum(total_amount)')\\\n",
    "            .drop('total_amount')\\\n",
    "                .withColumnRenamed('final_amount', 'total_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total_amount.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_1.withColumn(\"timestamp\", to_date(expr(\"concat(year, '-', month, '-', day)\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = df_2.withColumn('partition_key', lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_read_cassandra = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"total_amount_statistics\", keyspace=\"statistics\") \\\n",
    "    .load()\n",
    "    \n",
    "df_read_cassandra.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
